{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "05428d56-c073-4267-8190-b57703c7c092",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, precision_score, recall_score, roc_auc_score\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "\n",
    "import util_unsupervised as util_u\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "b242b71b-7d21-46b2-a66d-c67f96560e89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x150214757250>"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(764)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "31f674bf-c92e-4a68-84c5-e592de47fe9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net256(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(Net256, self).__init__()\n",
    "\n",
    "        # input layer takes the first 256 principal components \n",
    "        self.lin_in = nn.Linear(256, 512)\n",
    "        nn.init.xavier_uniform_(self.lin_in.weight)\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        # 1. hidden layer\n",
    "        self.hidden1 = nn.Linear(512, 128)\n",
    "        nn.init.xavier_uniform_(self.hidden1.weight)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        # 2. hidden layer\n",
    "        self.hidden2 = nn.Linear(128, 32)\n",
    "        nn.init.xavier_uniform_(self.hidden2.weight)\n",
    "        self.bn3 = nn.BatchNorm1d(32)\n",
    "        self.relu3 = nn.ReLU()\n",
    "\n",
    "        # 3. hidden layer\n",
    "        self.hidden3 = nn.Linear(128, 32)\n",
    "        nn.init.xavier_uniform_(self.hidden3.weight)\n",
    "        self.bn4 = nn.BatchNorm1d(32)\n",
    "        self.relu4 = nn.ReLU()\n",
    "\n",
    "        # 4. hidden layer\n",
    "        self.hidden4 = nn.Linear(64, 32)\n",
    "        nn.init.xavier_uniform_(self.hidden4.weight)\n",
    "        self.bn5 = nn.BatchNorm1d(32)\n",
    "        self.relu5 = nn.ReLU()\n",
    "        \n",
    "        # output layer\n",
    "        self.lin_out = nn.Linear(32, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input layer\n",
    "        x = self.relu1(self.bn1(self.lin_in(x)))\n",
    "\n",
    "        # first hidden layer\n",
    "        x = self.relu2(self.bn2(self.hidden1(x)))\n",
    "\n",
    "        # second hidden layer\n",
    "        x = self.relu3(self.bn3(self.hidden2(x)))\n",
    "\n",
    "        # x = self.relu4(self.bn4(self.hidden3(x)))\n",
    "\n",
    "        # x = self.relu5(self.bn5(self.hidden4(x)))\n",
    "        \n",
    "        # output layer. We don't add a softmax layer because we weill use\n",
    "        # cross entropy loss as a loss funtion\n",
    "        out = self.lin_out(x)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "72183359-35d1-48f2-a649-1322d68d005f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbedVec256Dataset(Dataset):\n",
    "    def __init__(self, df_dataset, label, data_prefix, label_dict):\n",
    "        self.df_data = df_dataset\n",
    "        self.data_label = label\n",
    "        self.data_prefix = data_prefix\n",
    "        self.data_cols = [c for c in self.df_data.columns if c.startswith(self.data_prefix)]\n",
    "        self.label_dict = label_dict\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.df_data.shape[0]\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        moa_label = self.df_data.loc[idx, self.data_label]\n",
    "        moa_tensor = torch.tensor(self.label_dict[moa_label])\n",
    "        label = F.one_hot(moa_tensor, num_classes=len(self.label_dict)).type(torch.FloatTensor)\n",
    "        \n",
    "        embed_vector = self.df_data.loc[idx, self.data_cols]\n",
    "        embed_tensor = torch.from_numpy(embed_vector.to_numpy().astype(np.float32))\n",
    "        \n",
    "        return embed_tensor, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "5fbc8210-c3c2-437b-9807-dbe7873b524d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"~/siads696/data\"\n",
    "\n",
    "random_state = 764\n",
    "n_classes = 13 # number of MoA\n",
    "\n",
    "learning_rate = 1e-3\n",
    "n_epochs = 25\n",
    "batch_size = 32\n",
    "chunk_print = 10\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "72799052-bf62-4538-ad3f-e4cbbcc6745b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding vector dataset shape: (3300, 261)\n",
      "Embedding vectors MoA assigned: 1540\n"
     ]
    }
   ],
   "source": [
    "df_data = pd.read_parquet(os.path.join(data_dir, \"well_grouped_256.parquet\"))\n",
    "print(f\"Embedding vector dataset shape: {df_data.shape}\")\n",
    "\n",
    "print(f\"Embedding vectors MoA assigned: {df_data[~df_data[\"Metadata_MoA\"].isnull()].shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "13139647-a46e-446b-88f3-30d0df401def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/test set shape: (39, 4)\n",
      "Training set shape: (29, 4)\n",
      "Test set shape: (10, 4)\n",
      "MoA in training set:\n",
      " ['Actin disruptors', 'Aurora kinase inhibitors', 'Cholesterol-lowering', 'DMSO', 'DNA damage', 'DNA replication', 'Eg5 inhibitors', 'Epithelial', 'Kinase inhibitors', 'Microtubule destabilizers', 'Microtubule stabilizers', 'Protein degradation', 'Protein synthesis']\n",
      "MoA in test set:\n",
      " ['Actin disruptors', 'Aurora kinase inhibitors', 'DNA damage', 'DNA replication', 'Epithelial', 'Kinase inhibitors', 'Microtubule destabilizers', 'Microtubule stabilizers', 'Protein degradation', 'Protein synthesis']\n",
      "Compounds in training set:\n",
      " ['cytochalasin B', 'cytochalasin D', 'AZ-A', 'AZ258', 'mevinolin/lovastatin', 'simvastatin', 'DMSO', 'chlorambucil', 'cisplatin', 'etoposide', 'camptothecin', 'floxuridine', 'methotrexate', 'AZ-C', 'AZ138', 'AZ-J', 'AZ-U', 'PD-169316', 'alsterpaullone', 'colchicine', 'demecolcine', 'nocodazole', 'docetaxel', 'epothilone B', 'ALLN', 'MG-132', 'lactacystin', 'anisomycin', 'cyclohexamide']\n",
      "Compounds in test set:\n",
      " ['latrunculin B', 'AZ841', 'mitomycin C', 'mitoxantrone', 'PP-2', 'bryostatin', 'vincristine', 'taxol', 'proteasome inhibitor I', 'emetine']\n"
     ]
    }
   ],
   "source": [
    "df_train_test = pd.read_csv(\"~/SIADS696_Milestone2/data/compound_moas_trainVtest.csv\")\n",
    "df_train = df_train_test[~df_train_test[\"in_testset\"]]\n",
    "df_test = df_train_test[df_train_test[\"in_testset\"]]\n",
    "print(f\"Train/test set shape: {df_train_test.shape}\")\n",
    "print(f\"Training set shape: {df_train.shape}\")\n",
    "print(f\"Test set shape: {df_test.shape}\")\n",
    "print(f\"MoA in training set:\\n {df_train[\"MoA\"].unique().tolist()}\")\n",
    "print(f\"MoA in test set:\\n {df_test[\"MoA\"].unique().tolist()}\")\n",
    "print(f\"Compounds in training set:\\n {df_train[\"Compound\"].unique().tolist()}\")\n",
    "print(f\"Compounds in test set:\\n {df_test[\"Compound\"].unique().tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "94b71196-8df5-482a-aca2-0eaa9a549fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (986, 265)\n",
      "Test data shape: (554, 265)\n",
      "Total embedding vectors in training/test set: 1540\n"
     ]
    }
   ],
   "source": [
    "df_data_train = df_data.merge(df_train, left_on=\"Metadata_Compound\", right_on=\"Compound\", how=\"inner\")\n",
    "print(f\"Training data shape: {df_data_train.shape}\")\n",
    "\n",
    "df_data_test = df_data.merge(df_test, left_on=\"Metadata_Compound\", right_on=\"Compound\", how=\"inner\")\n",
    "print(f\"Test data shape: {df_data_test.shape}\")\n",
    "\n",
    "print(f\"Total embedding vectors in training/test set: {df_data_train.shape[0]+df_data_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "2e0f6851-a047-4999-8398-a1911b4b39c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoA label dictionary:\n",
      "{'Protein degradation': 0, 'Aurora kinase inhibitors': 1, 'Eg5 inhibitors': 2, 'Epithelial': 3, 'DMSO': 4, 'Kinase inhibitors': 5, 'Protein synthesis': 6, 'DNA replication': 7, 'DNA damage': 8, 'Microtubule destabilizers': 9, 'Actin disruptors': 10, 'Microtubule stabilizers': 11, 'Cholesterol-lowering': 12}\n"
     ]
    }
   ],
   "source": [
    "moa_list = df_data[~df_data[\"Metadata_MoA\"].isnull()].loc[:, \"Metadata_MoA\"].unique().tolist()\n",
    "moa_dict = {moa: idx for moa, idx in zip(moa_list, range(len(moa_list)))}\n",
    "print(f\"MoA label dictionary:\\n{moa_dict}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "842b796e-25e0-4004-90fb-1a14caa21dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = EmbedVec256Dataset(df_data_train, \"Metadata_MoA\", \"PC\", moa_dict)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = EmbedVec256Dataset(df_data_test, \"Metadata_MoA\", \"PC\", moa_dict)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "4f85e551-6240-4e25-bac7-3097fba69406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_features, train_labels = next(iter(train_dataloader))\n",
    "# print(f\"Embedding vectors batch shape: {train_features.size()}\")\n",
    "# print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "# embed_vec = train_features[0]\n",
    "# label = train_labels[0]\n",
    "# print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "31faebe4-57e1-426c-be2b-8c0e39b0b43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net256(n_classes)\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "01a46a65-000d-4bb6-97a2-fa54584b7af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using cross entropy loss for multiclass classification\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "# use Adam as optimizer for this NN\n",
    "optimizer = torch.optim.Adam(model.parameters(), foreach=True, lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "27ff2559-98e1-442f-9c7e-696fe95bf6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# emb_vec = torch.rand(32, 256)\n",
    "# result = model(emb_vec)\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "94e537c9-3a2a-4d2d-b282-63eebd2fa4c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin Training\n",
      "Epoch [1/25], Step [10/30], Loss: 2.2822383642196655\n",
      "Epoch [1/25], Step [20/30], Loss: 1.9757553577423095\n",
      "Epoch [1/25], Step [30/30], Loss: 1.7114137530326843\n",
      "Epoch [2/25], Step [10/30], Loss: 1.4941835403442383\n",
      "Epoch [2/25], Step [20/30], Loss: 1.3351672649383546\n",
      "Epoch [2/25], Step [30/30], Loss: 1.2560648679733277\n",
      "Epoch [3/25], Step [10/30], Loss: 1.07661771774292\n",
      "Epoch [3/25], Step [20/30], Loss: 0.9832516789436341\n",
      "Epoch [3/25], Step [30/30], Loss: 0.9189135432243347\n",
      "Epoch [4/25], Step [10/30], Loss: 0.779290622472763\n",
      "Epoch [4/25], Step [20/30], Loss: 0.7072518289089202\n",
      "Epoch [4/25], Step [30/30], Loss: 0.655372929573059\n",
      "Epoch [5/25], Step [10/30], Loss: 0.5691891193389893\n",
      "Epoch [5/25], Step [20/30], Loss: 0.5067409306764603\n",
      "Epoch [5/25], Step [30/30], Loss: 0.49022342562675475\n",
      "Epoch [6/25], Step [10/30], Loss: 0.39623338282108306\n",
      "Epoch [6/25], Step [20/30], Loss: 0.37012653350830077\n",
      "Epoch [6/25], Step [30/30], Loss: 0.32341012060642244\n",
      "Epoch [7/25], Step [10/30], Loss: 0.2390507608652115\n",
      "Epoch [7/25], Step [20/30], Loss: 0.2611604601144791\n",
      "Epoch [7/25], Step [30/30], Loss: 0.24916557222604752\n",
      "Epoch [8/25], Step [10/30], Loss: 0.18372540920972824\n",
      "Epoch [8/25], Step [20/30], Loss: 0.19343271255493164\n",
      "Epoch [8/25], Step [30/30], Loss: 0.18123118430376053\n",
      "Epoch [9/25], Step [10/30], Loss: 0.1677246868610382\n",
      "Epoch [9/25], Step [20/30], Loss: 0.16160265430808068\n",
      "Epoch [9/25], Step [30/30], Loss: 0.12890465408563614\n",
      "Epoch [10/25], Step [10/30], Loss: 0.11309088692069054\n",
      "Epoch [10/25], Step [20/30], Loss: 0.10387915298342705\n",
      "Epoch [10/25], Step [30/30], Loss: 0.09738254994153976\n",
      "Epoch [11/25], Step [10/30], Loss: 0.08320540860295296\n",
      "Epoch [11/25], Step [20/30], Loss: 0.0718957707285881\n",
      "Epoch [11/25], Step [30/30], Loss: 0.07763516083359719\n",
      "Epoch [12/25], Step [10/30], Loss: 0.06547562479972839\n",
      "Epoch [12/25], Step [20/30], Loss: 0.062100600078701974\n",
      "Epoch [12/25], Step [30/30], Loss: 0.07188888788223266\n",
      "Epoch [13/25], Step [10/30], Loss: 0.05299625471234322\n",
      "Epoch [13/25], Step [20/30], Loss: 0.055776917934417726\n",
      "Epoch [13/25], Step [30/30], Loss: 0.04752168469130993\n",
      "Epoch [14/25], Step [10/30], Loss: 0.0488566504791379\n",
      "Epoch [14/25], Step [20/30], Loss: 0.046554983407258985\n",
      "Epoch [14/25], Step [30/30], Loss: 0.0404633492231369\n",
      "Epoch [15/25], Step [10/30], Loss: 0.037867069616913794\n",
      "Epoch [15/25], Step [20/30], Loss: 0.03651939146220684\n",
      "Epoch [15/25], Step [30/30], Loss: 0.04669732060283423\n",
      "Epoch [16/25], Step [10/30], Loss: 0.0336895314976573\n",
      "Epoch [16/25], Step [20/30], Loss: 0.030680455826222898\n",
      "Epoch [16/25], Step [30/30], Loss: 0.03619444258511066\n",
      "Epoch [17/25], Step [10/30], Loss: 0.036816349253058434\n",
      "Epoch [17/25], Step [20/30], Loss: 0.03125008475035429\n",
      "Epoch [17/25], Step [30/30], Loss: 0.02775396965444088\n",
      "Epoch [18/25], Step [10/30], Loss: 0.021805734559893607\n",
      "Epoch [18/25], Step [20/30], Loss: 0.0229403717443347\n",
      "Epoch [18/25], Step [30/30], Loss: 0.025618495792150496\n",
      "Epoch [19/25], Step [10/30], Loss: 0.023227053321897985\n",
      "Epoch [19/25], Step [20/30], Loss: 0.02033791784197092\n",
      "Epoch [19/25], Step [30/30], Loss: 0.024037742335349322\n",
      "Epoch [20/25], Step [10/30], Loss: 0.02048634774982929\n",
      "Epoch [20/25], Step [20/30], Loss: 0.0198490297421813\n",
      "Epoch [20/25], Step [30/30], Loss: 0.023030442558228968\n",
      "Epoch [21/25], Step [10/30], Loss: 0.029920480959117413\n",
      "Epoch [21/25], Step [20/30], Loss: 0.016498972196131945\n",
      "Epoch [21/25], Step [30/30], Loss: 0.02164160627871752\n",
      "Epoch [22/25], Step [10/30], Loss: 0.0145494076423347\n",
      "Epoch [22/25], Step [20/30], Loss: 0.015810519736260176\n",
      "Epoch [22/25], Step [30/30], Loss: 0.016723962593823673\n",
      "Epoch [23/25], Step [10/30], Loss: 0.015083039551973343\n",
      "Epoch [23/25], Step [20/30], Loss: 0.014563859719783068\n",
      "Epoch [23/25], Step [30/30], Loss: 0.014151637628674506\n",
      "Epoch [24/25], Step [10/30], Loss: 0.019142479728907348\n",
      "Epoch [24/25], Step [20/30], Loss: 0.017108921334147452\n",
      "Epoch [24/25], Step [30/30], Loss: 0.013520827423781157\n",
      "Epoch [25/25], Step [10/30], Loss: 0.014253294002264738\n",
      "Epoch [25/25], Step [20/30], Loss: 0.010438769403845072\n",
      "Epoch [25/25], Step [30/30], Loss: 0.014004335971549153\n",
      "Stop Training\n"
     ]
    }
   ],
   "source": [
    "print(\"Begin Training\")\n",
    "\n",
    "model.train()\n",
    "for epoch in range(n_epochs):\n",
    "    running_loss = 0.0\n",
    "    size = len(train_dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(train_dataloader):\n",
    "        \n",
    "        output = model(X)\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_func(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if (batch+1) % chunk_print == 0:\n",
    "          print(f\"Epoch [{epoch+1}/{n_epochs}], Step [{batch+1}/{size//batch_size}], Loss: {running_loss/chunk_print}\")\n",
    "          running_loss = 0.0\n",
    "\n",
    "\n",
    "print(\"Stop Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "8ea465dc-688d-43ab-9cc2-51b96b9f2aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 80.1%, Avg loss: 0.847818 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "size = len(test_dataloader.dataset)\n",
    "num_batches = len(test_dataloader)\n",
    "test_loss, correct = 0, 0\n",
    "\n",
    "# Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "# also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "with torch.no_grad():\n",
    "    for X, y in test_dataloader:\n",
    "        pred = model(X)\n",
    "        test_loss += loss_func(pred, y).item()\n",
    "        correct += (pred.argmax(1) == y.argmax(1)).type(torch.float).sum().item()\n",
    "\n",
    "test_loss /= num_batches\n",
    "correct /= size\n",
    "print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b6c30b-ada5-4acf-b691-3051f660b0ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45e8395-4259-4af4-8eb6-91e0cca4f9b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23a18cd-2654-4acd-94d5-836b13537b93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ms2",
   "language": "python",
   "name": "ms2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
